{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10dc0a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will fetch up to 10 songs per artist\n",
      "Will sleep 0.25 seconds between artist requests\n",
      "Using initial backoff of 5s with 5 max retries\n",
      "Loaded 3000 existing rows from scraped_lyrics.csv\n",
      "Complete artists (>= 10 songs): 300\n",
      "Incomplete artists (< 10 songs): 0\n",
      "Read 300 artists from artists.txt\n",
      "Will scrape 0 artists (skipping 300 completed artists)\n",
      "\n",
      "Scraping complete.\n",
      "Final row count: 3000\n",
      "Distinct artists in CSV: 300\n",
      "Distinct songs in CSV: 2851\n",
      "\n",
      "Top 10 artists by song count:\n",
      "artist\n",
      "21 Savage          10\n",
      "Paramore           10\n",
      "P!nk               10\n",
      "Otis Redding       10\n",
      "One Direction      10\n",
      "Oasis              10\n",
      "Norah Jones        10\n",
      "Nirvana            10\n",
      "Nine Inch Nails    10\n",
      "Nicki Minaj        10\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import lyricsgenius\n",
    "import time\n",
    "import random\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURATION\n",
    "# -----------------------------\n",
    "\n",
    "# Load environment variables from .env file (using absolute path for reliability)\n",
    "env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), '.env')\n",
    "load_dotenv(env_path, override=True, encoding='utf-8')\n",
    "\n",
    "# 1) Genius API Token\n",
    "GENIUS_API_TOKEN = os.getenv(\"GENIUS_API_TOKEN\")\n",
    "if not GENIUS_API_TOKEN:\n",
    "    raise ValueError(\"GENIUS_API_TOKEN not found in .env file\")\n",
    "\n",
    "# 2) Path to your artist list (one artist name per line)\n",
    "ARTIST_LIST_PATH = \"artists.txt\"\n",
    "\n",
    "# 3) Output CSV (where we'll append results as we go)\n",
    "OUTPUT_CSV = \"scraped_lyrics.csv\"\n",
    "\n",
    "\n",
    "# 4) How many songs to fetch per artist\n",
    "SONGS_PER_ARTIST = int(os.getenv(\"SONGS_PER_ARTIST\", \"25\"))\n",
    "print(f\"Will fetch up to {SONGS_PER_ARTIST} songs per artist\")\n",
    "\n",
    "# 5) Pause (seconds) between artist requests to avoid rate-limiting\n",
    "SLEEP_BETWEEN_ARTISTS = float(os.getenv(\"SLEEP_BETWEEN_ARTISTS\", \"1.5\"))\n",
    "print(f\"Will sleep {SLEEP_BETWEEN_ARTISTS} seconds between artist requests\")\n",
    "\n",
    "# 6) Rate limit handling configuration\n",
    "INITIAL_BACKOFF = int(os.getenv(\"INITIAL_BACKOFF\", 10))  # Start with 10 seconds\n",
    "MAX_RETRIES = int(os.getenv(\"MAX_RETRIES\", 5))       # Try up to 5 times\n",
    "\n",
    "print(f\"Using initial backoff of {INITIAL_BACKOFF}s with {MAX_RETRIES} max retries\")\n",
    "# -----------------------------\n",
    "# INITIALIZE GENIUS CLIENT\n",
    "# -----------------------------\n",
    "\n",
    "# Initialize lyricsgenius.Genius with some options\n",
    "genius = lyricsgenius.Genius(\n",
    "    GENIUS_API_TOKEN,\n",
    "    timeout=15,\n",
    "    retries=3,\n",
    "    sleep_time=0.25,  # small pause between each page scrape\n",
    "    excluded_terms=[\"(Remix)\", \"(Live)\"],  # Exclude these terms from song titles\n",
    "    skip_non_songs=True,  # Skip non-song entries (e.g., interviews)\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RATE LIMIT HANDLER\n",
    "# -----------------------------\n",
    "def with_rate_limit_handling(api_function):\n",
    "    \"\"\"Decorator to handle rate limit errors with exponential backoff\"\"\"\n",
    "    def wrapper(*args, **kwargs):\n",
    "        for attempt in range(MAX_RETRIES + 1):\n",
    "            try:\n",
    "                return api_function(*args, **kwargs)\n",
    "            except Exception as e:\n",
    "                error_str = str(e)\n",
    "                # Check if it's a rate limit error\n",
    "                if \"429\" in error_str and attempt < MAX_RETRIES:\n",
    "                    # Calculate backoff time with jitter\n",
    "                    backoff_time = INITIAL_BACKOFF * (2 ** attempt) + random.uniform(1, 5)\n",
    "                    print(f\"\\nRate limit exceeded. Waiting {backoff_time:.1f} seconds before retry {attempt+1}/{MAX_RETRIES}\")\n",
    "                    time.sleep(backoff_time)\n",
    "                else:\n",
    "                    if \"429\" in error_str:\n",
    "                        print(f\"\\nRate limit exceeded after {MAX_RETRIES} retries. Consider increasing wait time.\")\n",
    "                    raise\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HELPER FUNCTION: fetch_artist_lyrics\n",
    "# -----------------------------\n",
    "@with_rate_limit_handling\n",
    "def search_artist(artist_name, max_songs):\n",
    "    \"\"\"Search for an artist with rate limit handling\"\"\"\n",
    "    return genius.search_artist(artist_name, max_songs=max_songs, sort=\"popularity\")\n",
    "\n",
    "@with_rate_limit_handling\n",
    "def search_song(title, artist):\n",
    "    \"\"\"Search for a song with rate limit handling\"\"\"\n",
    "    return genius.search_song(title=title, artist=artist)\n",
    "\n",
    "\n",
    "# Add this function after your imports and before the GENIUS CLIENT section\n",
    "\n",
    "\n",
    "def fetch_artist_lyrics(artist_name, max_songs=SONGS_PER_ARTIST):\n",
    "    \"\"\"\n",
    "    Fetch up to max_songs tracks for `artist_name`, returning a list of dicts\n",
    "    \"\"\"\n",
    "    songs_data = []\n",
    "    try:\n",
    "        # Search for the artist with rate limit handling\n",
    "        artist_obj = search_artist(artist_name, max_songs)\n",
    "        \n",
    "        if artist_obj is None or not artist_obj.songs:\n",
    "            print(f\"  → No songs found for artist: {artist_name}\")\n",
    "            return songs_data\n",
    "\n",
    "        for song in artist_obj.songs:\n",
    "            title = song.title.strip()\n",
    "            lyrics = song.lyrics.strip()\n",
    "            \n",
    "            # Skip extremely short lyrics (e.g., < 20 chars)\n",
    "            if len(lyrics) < 20:\n",
    "                continue\n",
    "            songs_data.append({\n",
    "                \"artist\": artist_name,\n",
    "                \"song_title\": title,\n",
    "                \"lyrics\": lyrics\n",
    "            })\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Could not search for artist [{artist_name}]: {e}\")\n",
    "        \n",
    "    return songs_data\n",
    "\n",
    "def main():\n",
    "    # 1) Read existing CSV (if any), so we don't re‐scrape duplicates\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        master_df = pd.read_csv(OUTPUT_CSV, encoding='utf-8')\n",
    "        # master_df = safe_read_csv(OUTPUT_CSV)\n",
    "        # Create a set of (artist, song_title) for quick \"already scraped\" checks\n",
    "        existing_pairs = set(zip(master_df[\"artist\"], master_df[\"song_title\"]))\n",
    "        \n",
    "        # Check which artists have already met their quota\n",
    "        artist_song_counts = master_df.groupby('artist').size()\n",
    "        complete_artists = set(artist_song_counts[artist_song_counts >= SONGS_PER_ARTIST].index)\n",
    "        incomplete_artists = set(artist_song_counts[artist_song_counts < SONGS_PER_ARTIST].index)\n",
    "        \n",
    "        print(f\"Loaded {len(master_df)} existing rows from {OUTPUT_CSV}\")\n",
    "        print(f\"Complete artists (>= {SONGS_PER_ARTIST} songs): {len(complete_artists)}\")\n",
    "        print(f\"Incomplete artists (< {SONGS_PER_ARTIST} songs): {len(incomplete_artists)}\")\n",
    "    else:\n",
    "        master_df = pd.DataFrame(columns=[\"artist\", \"song_title\", \"lyrics\"])\n",
    "        existing_pairs = set()\n",
    "        complete_artists = set()\n",
    "        incomplete_artists = set()\n",
    "        print(f\"No existing CSV found. A new one will be created: {OUTPUT_CSV}\")\n",
    "\n",
    "    # 2) Read artist list\n",
    "    with open(ARTIST_LIST_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        artists = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Read {len(artists)} artists from {ARTIST_LIST_PATH}\")\n",
    "\n",
    "    # 3) Filter artists: skip complete ones, include incomplete and new ones\n",
    "    artists_to_scrape = [artist for artist in artists if artist not in complete_artists]\n",
    "    skipped_count = len(artists) - len(artists_to_scrape)\n",
    "    \n",
    "    print(f\"Will scrape {len(artists_to_scrape)} artists (skipping {skipped_count} completed artists)\")\n",
    "    if incomplete_artists:\n",
    "        print(f\"Resuming scraping for {len(incomplete_artists)} incomplete artists\")\n",
    "\n",
    "    # 4) Loop over each artist that needs scraping\n",
    "    for idx, artist_name in enumerate(artists_to_scrape, 1):\n",
    "        # Check if this is a resume case\n",
    "        if artist_name in incomplete_artists:\n",
    "            current_count = len([pair for pair in existing_pairs if pair[0] == artist_name])\n",
    "            remaining_needed = SONGS_PER_ARTIST - current_count\n",
    "            print(f\"[{idx}/{len(artists_to_scrape)}] Resuming artist: {artist_name} (has {current_count}, needs {remaining_needed} more) \", end=\"\")\n",
    "        else:\n",
    "            print(f\"[{idx}/{len(artists_to_scrape)}] Scraping new artist: {artist_name} \", end=\"\")\n",
    "        \n",
    "        fetched = fetch_artist_lyrics(artist_name, max_songs=SONGS_PER_ARTIST)\n",
    "\n",
    "        # Filter out any (artist, song) pairs we already have\n",
    "        new_rows = []\n",
    "        for item in fetched:\n",
    "            key = (item[\"artist\"], item[\"song_title\"])\n",
    "            if key in existing_pairs:\n",
    "                continue\n",
    "            new_rows.append(item)\n",
    "            existing_pairs.add(key)\n",
    "\n",
    "        # 5) Append new_rows to master_df (and save immediately)\n",
    "        if new_rows:\n",
    "            new_df = pd.DataFrame(new_rows)\n",
    "            master_df = pd.concat([master_df, new_df], ignore_index=True)\n",
    "\n",
    "            # Sort by artist for better organization\n",
    "            master_df = master_df.sort_values(['artist', 'song_title']).reset_index(drop=True)\n",
    "\n",
    "            # Save after each artist to avoid data loss if script crashes\n",
    "            master_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "            print(f\"→ Retrieved {len(new_rows)} new songs (total now {len(master_df)})\")\n",
    "        else:\n",
    "            print(\"→ No new songs found or all songs already exist.\")\n",
    "\n",
    "        # 6) Sleep to avoid hitting rate limits\n",
    "        time.sleep(SLEEP_BETWEEN_ARTISTS)\n",
    "\n",
    "    # Final sorting and statistics\n",
    "    master_df = master_df.sort_values(['artist', 'song_title']).reset_index(drop=True)\n",
    "    master_df.to_csv(OUTPUT_CSV, index=False, encoding='utf-8')\n",
    "\n",
    "    print(\"\\nScraping complete.\")\n",
    "    print(f\"Final row count: {len(master_df)}\")\n",
    "    print(f\"Distinct artists in CSV: {master_df['artist'].nunique()}\")\n",
    "    print(f\"Distinct songs in CSV: {master_df['song_title'].nunique()}\")\n",
    "    \n",
    "    # Show final artist statistics\n",
    "    final_artist_counts = master_df.groupby('artist').size().sort_values(ascending=False)\n",
    "    print(f\"\\nTop 10 artists by song count:\")\n",
    "    print(final_artist_counts.head(10))\n",
    "    \n",
    "    # Show artists that still need more songs\n",
    "    incomplete_final = final_artist_counts[final_artist_counts < SONGS_PER_ARTIST]\n",
    "    if len(incomplete_final) > 0:\n",
    "        print(f\"\\nArtists still needing more songs ({len(incomplete_final)} total):\")\n",
    "        print(incomplete_final.head(10))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f6cdcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CLEANING PREVIEW ===\n",
      "\n",
      "--- Sample 1: 21 Savage - Bank Account ---\n",
      "Original length: 4090 characters\n",
      "Cleaned length: 2553 characters\n",
      "Reduction: 1537 characters\n",
      "\n",
      "Original first 200 chars:\n",
      "'276 ContributorsTranslationsFrançaisРусский (Russian)PortuguêsBank Account Lyrics“Bank Account” sees 21 Savage rapping about his wealth amongst other things over a dark beat produced by Metro Boomin a...'\n",
      "\n",
      "Cleaned first 200 chars:\n",
      "'[Chorus]\n",
      "I got 1, 2, 3, 4, 5, 6, 7, 8 M's in my bank account, yeah (On God)\n",
      "In my bank account, yeah (On God)\n",
      "In my bank account, yeah (On God)\n",
      "In my bank account, yeah (On God)\n",
      "In my bank account, ye...'\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sample 2: 21 Savage - Ghostface Killers ---\n",
      "Original length: 5118 characters\n",
      "Cleaned length: 4795 characters\n",
      "Reduction: 323 characters\n",
      "\n",
      "Original first 200 chars:\n",
      "'190 ContributorsTranslationsFrançaisPortuguêsРусский (Russian)Ghostface Killers Lyrics21 Savage, Offset, and Travis Scott open up Without Warning with “Ghostface Killers”—an ode to why they aren’t to ...'\n",
      "\n",
      "Cleaned first 200 chars:\n",
      "'[Intro: Young Thug & Offset]\n",
      "Metro Boomin want some more, nigga (Hey)[Chorus: Offset]\n",
      "Automatic (Auto), automatics (Yeah), in the trunk (Pew)\n",
      "Shoot the maggots (Pew-pew), shoot the maggots with the pu...'\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Sample 3: 21 Savage - Glock in My Lap ---\n",
      "Original length: 3145 characters\n",
      "Cleaned length: 346 characters\n",
      "Reduction: 2799 characters\n",
      "\n",
      "Original first 200 chars:\n",
      "'97 ContributorsTranslationsРусский (Russian)PortuguêsEspañolGlock in My Lap LyricsOn this track, 21 Savage delivers a menacing banger about gang violence and his experience with lethal weapons. He alw...'\n",
      "\n",
      "Cleaned first 200 chars:\n",
      "'[Chorus]\n",
      "Glock in my lap, everywhere I'm strapped\n",
      "Most these rappers cap, I ain't givin' dap\n",
      "Glenwood to the flat, used to rob and trap\n",
      "Money tall, Shaq, choppa bullets, splat\n",
      "Chuck E. Cheese, rat, we...'\n",
      "------------------------------------------------------------\n",
      "Loading lyrics dataset from scraped_lyrics.csv...\n",
      "Original dataset: 3000 rows\n",
      "Cleaning lyrics...\n",
      "Removed 160 rows with insufficient lyrics after cleaning\n",
      "Cleaned dataset saved to scraped_lyrics_cleaned.csv\n",
      "Final dataset: 2840 rows\n",
      "Artists: 300\n",
      "Songs: 2705\n",
      "Loading dataset from scraped_lyrics.csv...\n",
      "Original dataset: 3000 rows\n",
      "Cleaning Genius metadata...\n",
      "Preprocessing for topic modeling...\n",
      "Tokenizing and removing stopwords...\n",
      "Removed 193 rows with insufficient content after processing\n",
      "Processed dataset saved to scraped_lyrics_topic_modeling_ready.csv\n",
      "Final dataset: 2807 rows\n",
      "Artists: 300\n",
      "Average words per song: 147.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "except:\n",
    "    print(\"NLTK downloads may have failed - some preprocessing features might not work\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLEANING AND PREPROCESSING FUNCTIONS\n",
    "# -----------------------------\n",
    "\n",
    "def clean_genius_metadata(lyrics_text):\n",
    "    \"\"\"Remove Genius website metadata and formatting from scraped lyrics\"\"\"\n",
    "    if not lyrics_text or pd.isna(lyrics_text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    lyrics_text = str(lyrics_text)\n",
    "    \n",
    "    # Find the first verse/chorus/structure marker and remove everything before it\n",
    "    structure_patterns = [\n",
    "        r'\\[Verse\\s*\\d*\\]',      # [Verse], [Verse 1], [Verse 2], etc.\n",
    "        r'\\[Chorus\\]',           # [Chorus]\n",
    "        r'\\[Intro\\]',            # [Intro]\n",
    "        r'\\[Pre-Chorus\\]',       # [Pre-Chorus]\n",
    "        r'\\[Bridge\\]',           # [Bridge]\n",
    "        r'\\[Outro\\]',            # [Outro]\n",
    "        r'\\[Refrain\\]',          # [Refrain]\n",
    "        r'\\[Hook\\]',             # [Hook]\n",
    "        r'\\[Part\\s*\\d*\\]',       # [Part], [Part 1], etc.\n",
    "        r'\\[Interlude\\]',        # [Interlude]\n",
    "        r'Lyrics:',              # \"Lyrics:\" marker\n",
    "    ]\n",
    "    \n",
    "    # Find the earliest occurrence of any structure marker\n",
    "    earliest_match = None\n",
    "    earliest_pos = len(lyrics_text)\n",
    "    \n",
    "    for pattern in structure_patterns:\n",
    "        match = re.search(pattern, lyrics_text, re.IGNORECASE)\n",
    "        if match and match.start() < earliest_pos:\n",
    "            earliest_pos = match.start()\n",
    "            earliest_match = match\n",
    "    \n",
    "    # If we found a structure marker, start from there\n",
    "    if earliest_match:\n",
    "        lyrics_text = lyrics_text[earliest_pos:]\n",
    "    \n",
    "    # Remove any remaining metadata patterns that might still be present\n",
    "    patterns_to_remove = [\n",
    "        r'\\d+\\s*Contributors.*?(?=\\[|$)',  # \"109 Contributors...\"\n",
    "        r'Translations.*?(?=\\[|$)',  # \"Translations...\" \n",
    "        r'.*?Lyrics\".*?describes.*?(?=\\[|$)',  # Song description text\n",
    "        r'.*?is a.*?(?=\\[|$)',  # \"This song is a...\"\n",
    "        r'Read More.*?(?=\\[|$)',  # \"Read More\" links\n",
    "        r'See .*? Live.*?(?=\\[|$)',  # Concert information\n",
    "        r'Get tickets.*?(?=\\[|$)',  # Ticket links\n",
    "        r'You might also like.*?(?=\\[|$)',  # Recommendations\n",
    "        r'Embed$',  # \"Embed\" at end\n",
    "        r'^\\d+Embed',  # Numbers + \"Embed\"\n",
    "        r'Produced by.*?$',  # Producer credits\n",
    "        r'\\[Produced by.*?\\]',  # Producer credits in brackets\n",
    "        r'^\".*?\" is.*?(?=\\[|$)',  # Song title descriptions like '\"Formation\" is a Black Power anthem'\n",
    "        r'^\".*?\" sees.*?(?=\\[|$)',  # '\"Fortnight\" sees Taylor...'\n",
    "        r'^\".*?\" presents.*?(?=\\[|$)',  # '\"Sorry\" presents itself...'\n",
    "        r'^Track.*?(?=\\[|$)',  # Track descriptions\n",
    "        r'^One of.*?(?=\\[|$)',  # \"One of Bey's most...\"\n",
    "        r'^Accompanied by.*?(?=\\[|$)',  # \"Accompanied by the release...\"\n",
    "        r'^Widely regarded.*?(?=\\[|$)',  # \"Widely regarded as...\"\n",
    "        r'^The title track.*?(?=\\[|$)',  # \"The title track of...\"\n",
    "    ]\n",
    "    \n",
    "    cleaned_lyrics = lyrics_text\n",
    "    for pattern in patterns_to_remove:\n",
    "        cleaned_lyrics = re.sub(pattern, '', cleaned_lyrics, flags=re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    # Additional cleanup: remove any text before the first bracket that might be leftover description\n",
    "    lines = cleaned_lyrics.split('\\n')\n",
    "    start_idx = 0\n",
    "    for i, line in enumerate(lines):\n",
    "        # Look for lines that start with brackets (song structure)\n",
    "        if re.match(r'^\\s*\\[.*?\\]', line.strip()):\n",
    "            start_idx = i\n",
    "            break\n",
    "        # Or look for lines that seem to be actual lyrics (not descriptions)\n",
    "        elif len(line.strip()) > 0 and not any(desc_word in line.lower() for desc_word in \n",
    "                                              ['describes', 'is a', 'sees', 'presents', 'track', 'song']):\n",
    "            start_idx = i\n",
    "            break\n",
    "    \n",
    "    if start_idx > 0:\n",
    "        cleaned_lyrics = '\\n'.join(lines[start_idx:])\n",
    "    \n",
    "    # Clean up song structure markers (keep them but normalize format)\n",
    "    cleaned_lyrics = re.sub(r'\\[([^\\]]+)\\]', r'[\\1]', cleaned_lyrics)\n",
    "    \n",
    "    # Remove extra whitespace and empty lines\n",
    "    cleaned_lyrics = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', cleaned_lyrics)\n",
    "    cleaned_lyrics = re.sub(r'^\\s+|\\s+$', '', cleaned_lyrics, flags=re.MULTILINE)\n",
    "    cleaned_lyrics = cleaned_lyrics.strip()\n",
    "    \n",
    "    return cleaned_lyrics\n",
    "def preprocess_lyrics_for_topic_modeling(lyrics_text):\n",
    "    \"\"\"\n",
    "    Preprocess lyrics text for topic modeling by removing song structure\n",
    "    and normalizing text (keeping natural expressions)\n",
    "    \"\"\"\n",
    "    if not lyrics_text or pd.isna(lyrics_text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = lyrics_text.lower()\n",
    "    \n",
    "    # Remove song structure markers [Verse 1], [Chorus], etc.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    \n",
    "    # Remove parentheses but keep the content inside\n",
    "    text = re.sub(r'[()]', '', text)\n",
    "    \n",
    "    # Remove punctuation except apostrophes (to keep contractions)\n",
    "    text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def advanced_text_preprocessing(text):\n",
    "    \"\"\"\n",
    "    Advanced preprocessing: tokenization, stopword removal, lemmatization\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Get English stopwords and add music-specific common words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        music_stopwords = {\n",
    "            'verse', 'chorus', 'bridge', 'outro', 'intro', 'refrain'\n",
    "        }\n",
    "        stop_words.update(music_stopwords)\n",
    "        \n",
    "        # Remove stopwords, short words, and non-alphabetic tokens\n",
    "        tokens = [\n",
    "            token for token in tokens \n",
    "            if token.lower() not in stop_words \n",
    "            and len(token) > 2 \n",
    "            and token.isalpha()\n",
    "        ]\n",
    "        \n",
    "        # Lemmatization\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    except:\n",
    "        # Fallback if NLTK fails\n",
    "        words = text.split()\n",
    "        return [word for word in words if len(word) > 2 and word.isalpha()]\n",
    "\n",
    "def clean_lyrics_dataset(csv_path, output_path=None):\n",
    "    \"\"\"Clean an entire CSV dataset of scraped lyrics\"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = csv_path.replace('.csv', '_cleaned.csv')\n",
    "    \n",
    "    print(f\"Loading lyrics dataset from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Original dataset: {len(df)} rows\")\n",
    "    \n",
    "    # Clean lyrics column\n",
    "    print(\"Cleaning lyrics...\")\n",
    "    df['lyrics_cleaned'] = df['lyrics'].apply(clean_genius_metadata)\n",
    "    \n",
    "    # Remove rows with very short lyrics after cleaning\n",
    "    original_count = len(df)\n",
    "    df = df[df['lyrics_cleaned'].str.len() >= 50]  # At least 50 characters\n",
    "    print(f\"Removed {original_count - len(df)} rows with insufficient lyrics after cleaning\")\n",
    "    \n",
    "    # Replace original lyrics with cleaned version\n",
    "    df['lyrics'] = df['lyrics_cleaned']\n",
    "    df = df.drop('lyrics_cleaned', axis=1)\n",
    "    \n",
    "    # Sort and reset index\n",
    "    df = df.sort_values(['artist', 'song_title']).reset_index(drop=True)\n",
    "    \n",
    "    # Save cleaned dataset\n",
    "    df.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Cleaned dataset saved to {output_path}\")\n",
    "    print(f\"Final dataset: {len(df)} rows\")\n",
    "    print(f\"Artists: {df['artist'].nunique()}\")\n",
    "    print(f\"Songs: {df['song_title'].nunique()}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_and_preprocess_dataset(csv_path, output_path=None):\n",
    "    \"\"\"\n",
    "    Complete pipeline to clean and preprocess lyrics dataset for topic modeling\n",
    "    \"\"\"\n",
    "    if output_path is None:\n",
    "        output_path = csv_path.replace('.csv', '_topic_modeling_ready.csv')\n",
    "    \n",
    "    print(f\"Loading dataset from {csv_path}...\")\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Original dataset: {len(df)} rows\")\n",
    "    \n",
    "    # Step 1: Clean metadata\n",
    "    print(\"Cleaning Genius metadata...\")\n",
    "    df['lyrics_clean'] = df['lyrics'].apply(clean_genius_metadata)\n",
    "    \n",
    "    # Step 2: Preprocess for topic modeling\n",
    "    print(\"Preprocessing for topic modeling...\")\n",
    "    df['lyrics_processed'] = df['lyrics_clean'].apply(preprocess_lyrics_for_topic_modeling)\n",
    "    \n",
    "    # Step 3: Advanced tokenization and cleaning\n",
    "    print(\"Tokenizing and removing stopwords...\")\n",
    "    df['lyrics_tokens'] = df['lyrics_processed'].apply(advanced_text_preprocessing)\n",
    "    \n",
    "    # Step 4: Create final processed text (rejoined tokens)\n",
    "    df['lyrics_final'] = df['lyrics_tokens'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Remove rows with very short processed text\n",
    "    original_count = len(df)\n",
    "    df = df[df['lyrics_final'].str.len() >= 50]  # At least 50 characters\n",
    "    print(f\"Removed {original_count - len(df)} rows with insufficient content after processing\")\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    df_final = df[['artist', 'song_title', 'lyrics_final']].copy()\n",
    "    df_final.rename(columns={'lyrics_final': 'lyrics'}, inplace=True)\n",
    "    \n",
    "    # Sort and reset index\n",
    "    df_final = df_final.sort_values(['artist', 'song_title']).reset_index(drop=True)\n",
    "    \n",
    "    # Save processed dataset\n",
    "    df_final.to_csv(output_path, index=False, encoding='utf-8')\n",
    "    \n",
    "    print(f\"Processed dataset saved to {output_path}\")\n",
    "    print(f\"Final dataset: {len(df_final)} rows\")\n",
    "    print(f\"Artists: {df_final['artist'].nunique()}\")\n",
    "    print(f\"Average words per song: {df_final['lyrics'].str.split().str.len().mean():.1f}\")\n",
    "    \n",
    "    return df_final\n",
    "\n",
    "def preview_cleaning(csv_path, num_samples=3):\n",
    "    \"\"\"Preview what the cleaning function will do\"\"\"\n",
    "    df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "    \n",
    "    print(\"=== CLEANING PREVIEW ===\")\n",
    "    \n",
    "    for i in range(min(num_samples, len(df))):\n",
    "        row = df.iloc[i]\n",
    "        original = row['lyrics']\n",
    "        cleaned = clean_genius_metadata(original)\n",
    "        \n",
    "        print(f\"\\n--- Sample {i+1}: {row['artist']} - {row['song_title']} ---\")\n",
    "        print(f\"Original length: {len(original)} characters\")\n",
    "        print(f\"Cleaned length: {len(cleaned)} characters\")\n",
    "        print(f\"Reduction: {len(original) - len(cleaned)} characters\")\n",
    "        \n",
    "        print(f\"\\nOriginal first 200 chars:\")\n",
    "        print(f\"'{original[:200]}...'\")\n",
    "        \n",
    "        print(f\"\\nCleaned first 200 chars:\")\n",
    "        print(f\"'{cleaned[:200]}...'\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "\n",
    "preview_cleaning(OUTPUT_CSV, num_samples=3)  # Preview cleaning\n",
    "cleaned_df = clean_lyrics_dataset(OUTPUT_CSV)  # Clean dataset\n",
    "processed_df = clean_and_preprocess_dataset(OUTPUT_CSV) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
